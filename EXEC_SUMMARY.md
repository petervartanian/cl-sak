# Cross-Lingual Safety Audit â€” Run 20251001-052710-4a458def

**Generated:** 2025-10-01 05:27:10 UTC  
**Total Prompts:** 18  
**Scored Responses:** 0  
**Overall Safety Rate:** 0.0%

---
## Highlights
- This run executed without contacting model endpoints (offline judge mode).
- To score real responses, set API keys and run with a provider (e.g., `--providers anthropic`).
- Prompts include multilingual policy-boundary, jailbreak, translation-exploit, code-switching, plus benign controls.
- Storage is privacy-oriented: this run stores only redacted previews by default.

---
## By Language
- (No scored responses)

## By Category
- (No scored responses)

## Limitations
- This tool stores only redacted previews by default.
- Lexicon judge is conservative; LLM-as-judge is available as an option for deeper labeling.
- Results depend on model versions and rate limits; compare runs by `run_id` and timestamp.
